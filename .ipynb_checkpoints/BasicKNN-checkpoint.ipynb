{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean as euc\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder for KNN class with 2 methods\n",
    "class KNN(object):\n",
    "\n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def predict():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that when \"fitting\" a KNN classifier, all you're really doing is storing\n",
    "#the points and their corresponding labels. There's no actual \"fitting\" involved here, \n",
    "#since all you can do is store the data so that you can use it to \n",
    "#calculate the nearest neighbors when the predict method is called.\n",
    "\n",
    "def fit(self, X_train, y_train):\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train \n",
    "    \n",
    "# This line updates the knn.fit method to point to the function we've just written\n",
    "KNN.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, write two helper functions to make things easier when completing the predict function. \n",
    "# The first helper function should return an array containing \n",
    "# the distance between a point we pass in and every point inside of X_train.\n",
    "\n",
    "def _get_distances(self, x):\n",
    "    distances = []\n",
    "    for ind, val in enumerate(self.X_train):\n",
    "        dist_to_i = euc(x, val)\n",
    "        distances.append((ind, dist_to_i))\n",
    "    return distances\n",
    "    \n",
    "\n",
    "# This line attaches the function we just created as a method to our KNN class.\n",
    "KNN._get_distances = _get_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second big challenge in a predict method is getting the indices of the k-nearest points. \n",
    "# To keep our coming predict method nice and clean, \n",
    "# abstract this functionality into a helper method called _get_k_nearest.\n",
    "\n",
    "def _get_k_nearest(self, dists, k):\n",
    "    sorted_dists = sorted(dists, key=lambda x: x[1])\n",
    "    return sorted_dists[:k]\n",
    "\n",
    "# This line attaches the function we just created as a method to our KNN class.\n",
    "KNN._get_k_nearest = _get_k_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you have helper functions to help you get the distances, and then get the k-nearest neighbors \n",
    "# based on those distances. The final helper function you'll create will get the labels that correspond \n",
    "# to each of the k-nearest point, and return the class that occurs the most.\n",
    "\n",
    "def _get_label_prediction(self, k_nearest):\n",
    "    labels = [self.y_train[i] for i, _ in k_nearest]\n",
    "    counts = np.bincount(labels)\n",
    "    return np.argmax(counts)\n",
    "\n",
    "# This line attaches the function we just created as a method to our KNN class.\n",
    "KNN._get_label_prediction = _get_label_prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method does all the heavy lifting for KNN, so this will be a bit more complex than the fit method. Here's a rough outline of how the method should work:\n",
    "The function takes in an array of vectors that we want predictions for.\n",
    "For each vector that we want to make a prediction for: 1a. The classifier calculates the distance between that vector and every other vector in the training set. 1b. The classifier identifies the K nearest vectors to the vector you want a prediction for. 1c. The classifier determines which label the majority of the K nearest neighbors share, and appends this prediction to an array we will output. The index of the prediction in this array should be the same as the index of the point that it corresponds to (e.g. pred[0] is the prediction for X_test[0]).\n",
    "Once predictions have been generated for every vector in question, return the array of predictions.\n",
    "This tells us a few things about what our predict function will need to be able to do:\n",
    "In addition to self, our predict function should take in two arguments:\n",
    "X_test, the points we want to classify\n",
    "k, which specifies the number of neighbors we should use to make the classification. We'll set k=3 as a default, but allow the user to update it if they choose.\n",
    "Your method will need to iterate through every item in X_test. For each item:\n",
    "Calculate the distance to all points in X_train by using our _get_distances() helper method we created.\n",
    "Find the k-nearest points in X_train by using the _get_k_nearest() helper method we created\n",
    "Use the index values contained within the tuples returned by _get_k_nearest() to get the corresponding labels for each of the nearest points.\n",
    "Determine which class is most represented in these labels and treat that as the prediction for this point. Append the prediction to preds.\n",
    "Once a prediction has been generated for every item in X_test, return preds'''\n",
    "\n",
    "def predict(self, X_test, k=3):\n",
    "    preds = []\n",
    "    # Iterate through each item in X_test\n",
    "    for i in X_test:\n",
    "        # Get distances between i and each item in X_train\n",
    "        dists = self._get_distances(i)\n",
    "        k_nearest = self._get_k_nearest(dists, k)\n",
    "        predicted_label = self._get_label_prediction(k_nearest)\n",
    "        preds.append(predicted_label)\n",
    "    return preds\n",
    "                                       \n",
    "    \n",
    "        \n",
    "KNN.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tetsing on iris dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25)\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: {}\".format(accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
